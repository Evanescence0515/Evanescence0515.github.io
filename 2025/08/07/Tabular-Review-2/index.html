<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Tabular-Review-2 | Evanescence's Blog</title><meta name="author" content="Henry"><meta name="copyright" content="Henry"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="A Survey on Self-Supervised Learning for Non-Sequential Tabular Data自监督学习(Self-Supervised Learn: SSL)：SSL在深度学习领域内主要用于基于无标签的数据集来学习富有内涵与鲁棒性的表示。前置任务(Pretext Task): 为了达到训练任务而设计的间接任务：例如使用AE(Auto-Encoder)对图">
<meta property="og:type" content="article">
<meta property="og:title" content="Tabular-Review-2">
<meta property="og:url" content="http://evanescence0515.github.io/2025/08/07/Tabular-Review-2/index.html">
<meta property="og:site_name" content="Evanescence&#39;s Blog">
<meta property="og:description" content="A Survey on Self-Supervised Learning for Non-Sequential Tabular Data自监督学习(Self-Supervised Learn: SSL)：SSL在深度学习领域内主要用于基于无标签的数据集来学习富有内涵与鲁棒性的表示。前置任务(Pretext Task): 为了达到训练任务而设计的间接任务：例如使用AE(Auto-Encoder)对图">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://evanescence0515.github.io/img/deeplearning_pic.png">
<meta property="article:published_time" content="2025-08-07T11:12:31.000Z">
<meta property="article:modified_time" content="2025-08-07T11:12:31.000Z">
<meta property="article:author" content="Henry">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="论文阅读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://evanescence0515.github.io/img/deeplearning_pic.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Tabular-Review-2",
  "url": "http://evanescence0515.github.io/2025/08/07/Tabular-Review-2/",
  "image": "http://evanescence0515.github.io/img/deeplearning_pic.png",
  "datePublished": "2025-08-07T11:12:31.000Z",
  "dateModified": "2025-08-07T11:12:31.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "Henry",
      "url": "http://evanescence0515.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://evanescence0515.github.io/2025/08/07/Tabular-Review-2/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Tabular-Review-2',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-color: #EEEEEE;"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/my_icon.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tools/"><i class="fa-fw fas fa-tools"></i><span> Tools</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/deeplearning_pic.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Evanescence's Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Tabular-Review-2</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/tools/"><i class="fa-fw fas fa-tools"></i><span> Tools</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Tabular-Review-2</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-07T11:12:31.000Z" title="发表于 2025-08-07 19:12:31">2025-08-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-07T11:12:31.000Z" title="更新于 2025-08-07 19:12:31">2025-08-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.01204">A Survey on Self-Supervised Learning for Non-Sequential Tabular Data</a><br>自监督学习(Self-Supervised Learn: SSL)：SSL在深度学习领域内主要用于基于无标签的数据集来学习富有内涵与鲁棒性的表示。<br>前置任务(Pretext Task): 为了达到训练任务而设计的间接任务：例如使用AE(Auto-Encoder)对图像进行编码来获取图片的分布，再进行图片的分类。<br>近期SSL在表格数据中崭露头角，这篇文章主要用于总结近期SSL在非序列表格领域的进展和挑战(SSL4NS-TD)。这里的NS-TD（非序列型表格数据）指的是：数据间无关联，无时间或者其他顺序。作者按照一下顺序安排文章：1. NS-TD的定义，与其他研究的相关性；2. 将方法分为3类：预测学习、对比学习、复合学习。各个方法的出发点和在每个领域内的优势；3. SSL4NS-TD的应用；4. 各个方法的比较；5. SSL4NS-TD的挑战并提出可能的方向。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>SSL的主要优点就是其能够减少对于大规模数据的标注。SSL不同于传统的使用标签来训练，其通过前置任务学习任务无偏(Task-agnostic)的数据表示，来创造显式（例如预测学习）的或者隐式的标签（对比学习）。模型被期望从无标注的表格数据中学会普适性的表示，并能适应性的应用于下游任务中。总的来说，现有的有关SSL的表格数据技术可分为序列型与非序列型。作者在这篇文章中主要集中于非序列型(SSL4NS-TD)。</p>
<h1 id="Problem-Definition-of-SSL4NS-TD"><a href="#Problem-Definition-of-SSL4NS-TD" class="headerlink" title="Problem Definition of SSL4NS-TD"></a>Problem Definition of SSL4NS-TD</h1><p>不同于序列表格数据，非序列表格数据没有一个确定的顺序，在对于NS-TD使用SSL时，首先需要构建一个编码器函数$e: X\to Z$。其中$Z$代表从自监督学习任务中学到的上下文表示(Contextualized representation)。值得注意的是，自监督学习任务中的标签来自于数据本身而不是手动的标注。编码器函数与下游任务的模型结合可以更好的预测结果。</p>
<h1 id="Taxonomy"><a href="#Taxonomy" class="headerlink" title="Taxonomy"></a>Taxonomy</h1><ol>
<li>SSL4NS-TD的预测学习：最常用的类别，由于特征的异质性，在预测最终结果之前设计预测任务让模型可以学习到原始数据中的背景知识。但是难点在于设计有效的预测前置任务，这些任务需要考虑上游与下游数据集和任务之间的关系。虽然并没有设计预测性前置任务的共识，但是有许多范式被提出：1. 从掩码特征中学习；2. 在潜在空间中进行扰动；3. 利用预训练语言模型的固有能力，</li>
<li>SSL4NS-TD的对比学习：旨在学习表格数据中样本的相似性与差异性。优点就是提供了一种任务无偏的学习策略，并且可以应用于广泛的下游任务与迁移学习而仅需要少数的标注样本。但是，挑战在于如何规定样本的“远”与”近”。规定方法简要列举如下：1.  基于实例的；2. 基于模型的；3. 基于列特征的；3. 基于潜空间的。</li>
<li>SSL4NS-TD的复合学习：将以上两种方法结合的学习方法。当前主要有两类方法：1. 扰动+对比学习；2. 掩码+对比学习。</li>
</ol>
<h1 id="预测学习"><a href="#预测学习" class="headerlink" title="预测学习"></a>预测学习</h1><p>这个方法来自于使用于拥有同质性特征的领域的SSL，例如将干扰、旋转、裁剪和引入噪声作为前置任务。表格数据的SSL也是基于以上方法来设计前置任务的。当模型可以通过被遮掩或者污染的数据获取原始数据，那么该模型在下游任务中也是有效的。这里我们可以给出一个概括性的公式：</p>
<p>$$<br>\begin{aligned}<br>L_{predictive} &amp;&#x3D; \psi(g(e(x_{i}^*)), y_{i}^*)\\<br>x_{i}^* , y_{i}^* &amp;&#x3D; \delta(x_{i})<br>\end{aligned}<br>$$</p>
<p>$\psi$： 损失函数，用于优化转换后的$x_{i}^*$和自监督标签$y_{i}^*$，这两样东西是由$\delta$转化而来。$g$代表将编码器$e$产生的嵌入映射为自监督预测的映射。</p>
<h2 id="从遮掩的特征中学习"><a href="#从遮掩的特征中学习" class="headerlink" title="从遮掩的特征中学习"></a>从遮掩的特征中学习</h2><p>遮掩部分特征让模型可以学习到样本的上下文信息，这与下游训练中预测样本对应的类别&#x2F;样本的值的任务相一致。这种一致为训练好的编码器提供了在下游任务中，从给定样本特征中推理的知识。受启发于MAE(Masked Autoencoder)使用的随机掩码，<strong>TabTransformer</strong>和<strong>VIME</strong>将从一个污染的或者遮掩过的数据中还原数据作为前置任务，同时还构建了一个框架来泛化到所有表格数据。其中<strong>TabTransformer</strong>引入了随机掩码和随机值取代作为转化函数。<strong>VIME</strong>通过掩码向量估计器来识别被遮掩的特征并同时使用特征向量估计器基于相关的未被遮掩的特征进行插值。举个掩码向量估计器的例子，如果某个特征与和他相关的向量非常不协调，这个特征很可能就是被遮掩的。VIME的二元掩码来自于伯努利分布。<br>为了鼓励编码器产生更加结构化且有代表性的嵌入，以进一步提高<strong>VIME</strong>，<strong>TabNet</strong>设计了一个注意力机制来迭代地选择遮掩特征，让深度学习模型具有可解释性。于可学习的掩码相反。<strong>SEFS</strong>提出了一种特征子集生成器作为变换函数，来提高遮掩高度相关的特征的概率。<strong>SwitchTab</strong>在<strong>VIME</strong>自监督任务的基础上使用了非对称的编码-解码器架构，并提出了一种转移机制来对两个样本进行解耦：每个样本包括有共同信息（每个样本都有的，可交换的）和显著信息（每个样本特有的）。因此，对比的目标就是从共有的信息和独有的信息中获取原始特征。除了这些进展，掩码的占比是难以决定且需要根据任务调整。</p>
<h2 id="潜空间的扰动"><a href="#潜空间的扰动" class="headerlink" title="潜空间的扰动"></a>潜空间的扰动</h2><p>为了从表格数据的异质性特征中学习概括性的上下文信息，<strong>STUNT</strong>从未标记的数据中，“元学习”自己生成的任务，这种想法起源于：列可能与下游的标签有相关性(例如：工作的特征可能与收入有关，并且可以作为收入的替代)。这样一个转化函数遮掩部分特征，然后使用k-means算法产生伪标签。基于元学习的框架，<strong>STUNT</strong>在小样本表格数据中是高效的。<strong>LFR</strong>探索了随机映射器在缺少前置知识来增广数据的情况下，学习未标注数据并担任通用框架以合并多模态数据。但是这种方法并没有在有充足前置知识的情况下优秀。</p>
<h2 id="预训练语言模型的固有能力"><a href="#预训练语言模型的固有能力" class="headerlink" title="预训练语言模型的固有能力"></a>预训练语言模型的固有能力</h2><p>从另一个角度来解决特征异质性的问题：使用大语言模型作为编码器让知识可以在不同数据集之间迁移。但是难点在于如何将表格数据转化为自然语言的格式，多种预训练的语言模型被应用于表格数据：多个工作直接将数值作为字符串输入，这种方法虽然很直接，但是也有效减轻的预处理的压力。为了让语言模型理解数值变量，<strong>TP-BERTa</strong>提出了使用相对量纲分词技术(relative magnitude tokenization)通过使用决策树将数值变量分箱来将标量转化为离散的tokens，接着将embedding与原数值相乘来避免大量的值聚集在一块。为了减少特征的顺序偏倚，<strong>GReaT</strong>将表格和文本使用一个文本编码模式连接并使用随机插入。例如将$\text{age}&#x3D;26;\text{income}&#x3D;70$转化为“age is 26, income is 70k”。然后再改变顺序为”income is 70k, age is 26”</p>
<h1 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h1><p>这些进展的另一个共同主题是，通过同一输入的不同视角或扰动来学习鲁棒的表示，这一目标通过最大化相似实例之间的相似性、并拉远不相似实例之间的距离来实现。对比学习在CV领域和NLP领域获得了极大的成果，在表格数据领域，对比学习被使用来学习高效且泛化性能强的任务无偏的表示。对比学习可以被如下定义：<br>$$<br>\begin{aligned}<br>&amp;L_{contrastive}&#x3D;\phi(h_{i}, h_{i}^+, h_{j}^-)\<br>&amp;h_{i}&#x3D;e(x_{i});h_{i}^+&#x3D;e(x_{i}^+);h_{j}^-&#x3D;e(x_{j}^-)<br>\end{aligned}<br>$$<br>其中$\phi$是相似性函数，其比较锚点(anchor: $h_{i}$)、正例(positive: $h_{i}^+$)、反例(negative: $h_{i}^-$)。正例是特定样本锚点通过增广方式产生，负样本是来自于其他样本。正样本对代表相似、负样本对代表不相似。总体上来说，相似函数可以选择余弦相似度、欧几里得距离或者点乘。注意正样本对和负样本对并不是都需要加在loss里面。如果任务需要自监督的标签，则设置在$e$输出后使用的映射头$g$.<br><strong>SCARF</strong>是一个基于MLP的框架，其具有2阶段学习策略：<strong>InfoNCE对比预训练</strong>和<strong>有监督的微调</strong>。它加强了多种表格领域的泛化能力。在与训练阶段，其输入会被随机破坏，具体做法是将某部分特征随即替换为对应特征的边缘分布中的随机视图，从而构建正样本对和负样本对。接着，使用<strong>InfoNCE</strong>相关函数让正样本对更加靠近，负样本对更加远：<br>$$\phi_{\text{InfoNCE}}(h_{i},h_{i}^+,h_{j}^-)&#x3D;\log\left( \frac{\exp\left( \frac{h_{i}h_{i}^+}{\tau} \right)}{\sum_{j&#x3D;1}^N\exp(\frac{h_{i}h_{j}^-}{\tau})} \right)$$<br>$\tau$为温度参数。<br>与<strong>SCARF</strong>相反，<strong>STab</strong>旨在引入一种不使用增广的自监督表示学习技术，这种技术不需要负样本对。<strong>STab</strong>将输入样本使用两个MLP编码器，其中一个有一个多出的映射头，两个MLP编码器有相同的权重但是有不同的随机正则化。这可以被看作是一种基于模型的对比学习。然后比较负余弦距离作为相似函数。这两个MLP得出的表示被认为是各自的干扰样本。为了学习可被用于迁移学习、增量学习、零样本推理的上下文信息（表格间的列名是不同的），<strong>TransTab</strong>将表格中的列和单元格上下文化（例如将gender设置为woman而不是1、2）,其使用了Transformer编码器，并在多个表格中使用垂直划分对比学习（Vertical-partition contrastive learning：一种基于列划分表格的对比学习方法）进行预训练。Ye等人提出了一种基于原型的表格数据学习框架，用于围绕全局数据原型学习可解缠的表示。该方法引入全局原型来对抗相似样本，同时在潜在空间中通过多样化约束保留原始的差异性信息。</p>
<h1 id="复合学习"><a href="#复合学习" class="headerlink" title="复合学习"></a>复合学习</h1><p>将预测学习和对比学习结合的一种策略。一般来说，使用复合学习模型的方法需要多种映射头来处理不同的前置任务，多个映射头可以被并行使用来保证模型的鲁棒性。复合学习的损失函数如下定义：<br>$$L_{hybrid}&#x3D;L_{predictive}+L_{contrastive}$$<br>多种方法被使用来优化$L_{hybrid}$，包括干扰+对比学习、掩码＋对比学习。</p>
<h2 id="干扰-对比学习"><a href="#干扰-对比学习" class="headerlink" title="干扰+对比学习"></a>干扰+对比学习</h2><p>干扰＋对比学习可以在没有前置知识的前提下，学习到鲁棒性的表示与列、行乃至于单元格之间的上下文关系。除了重建损失函数外，<strong>SubTab</strong>将表格数据分为多个子集，每个子集有潜在的重叠的列作为不同的视图来进行对比损失和距离损失（类似于图片中的剪切，两种都是让同一个样本的子集更加靠近）。为了不均衡地扰动特征以进行特征重建，<strong>SubTab</strong>在以下3样东西上通过伯努利掩码加入了高斯噪声：1. 随机列；2. 临近列的随即区域；3. 样本中的随机特征。为了避免相似的特征在重建的损失中权重过大，Chen将损失函数与正则化矩阵结合。在分类标签的辅助下，他们使用了不同的视图进行有监督对比学习来最大化相同类的相似度并使用半监督学习来预训练Transformer模型。<br>除了在表格数据中使用了Transformer架构，研究者也开始将NS-TD作为token框架化，这在NLP及CV领域非常常用。多种变体被使用来捕捉表格数据中的更细颗粒度的表示（例如单元格级、数值型、类别型特征）。主要的优点就是表示可以在不同的数据集之间共用，并且可以通过自监督机制来建模。<strong>SAINT</strong>通过一组序列来描述特征，这组序列由数值变量和类别变量对应的部分组合而成，并在开头加上一个[CLS]的特殊token（就像<strong>BERT</strong>一样）。为了从其他相似的样本中建模不变的细颗粒度特征表示，<strong>SAINT</strong>将类别变量和数值变量嵌入并通过跨样本注意力机制在不同行间编码，最后使用重建损失和InfoNCE对比损失使用嵌入空间内的增广来进行预训练。<br>相较于已有工作中使用预训练并在下游数据集中进行微调，另外一个重要的角度是在大量数据集上进行预训练，这提供在下游任务中作为基础模型的能力，就像NLP中的Chatgpt。<strong>XTab</strong>是一个广义的表格数据Transformer预训练模型，其在大量且多样化的交叉表数据中进行训练，并且足够灵活来使用已有的编码器主干和自监督策略。<strong>UniTabE</strong>是一个在大规模多领域数据上进行训练的Transformer架构。解码器使用了自由格式和特定任务的提示词和来自编码器的上下文表示，从而能够进行自适应地任务定制化的推理。也就是说，提示词可以被修改来适应特定的下游任务。<strong>UniTabE</strong>的预训练任务包括了多单元格遮掩来重构样本的部分单元格和对比学习。</p>
<h2 id="掩码-对比学习"><a href="#掩码-对比学习" class="headerlink" title="掩码+对比学习"></a>掩码+对比学习</h2><p>相较于干扰留了部分数据信息，掩码直接遮掩了目标特征。为了适应上下游任务中不同的特征，Levin使用了一种基于已有深度表格模型的伪特征方法来进行预训练，并利用了对比预训练策略（就类似于在2k个高质量交叉表数据集中使用遮掩表格来学习特征间的潜在关联和相同类别样本的聚类）。分析得出：预训练为模型提供了相较于树模型更强的迁移能力。与特征无偏的SSL方法相反，<strong>DoRA</strong>关注于在金融领域基于特定知识设计一个特定的前置任务。它通过在预训练阶段选择领域内特定的特征作为自监督学习的标签，引入了样本内前置任务（例如，预测目标城镇的地理位置）。样本间对比学习则是基于特定知识引入不相似的样本来进行对比学习（例如相同城镇的房子会更加接近）。</p>
<h1 id="SSL4NS-TD的应用"><a href="#SSL4NS-TD的应用" class="headerlink" title="SSL4NS-TD的应用"></a>SSL4NS-TD的应用</h1><h2 id="自动数据工程"><a href="#自动数据工程" class="headerlink" title="自动数据工程"></a>自动数据工程</h2><p>深度学习缓解了特征工程的压力，但是在各领域稳定的表现依旧是一个挑战，因为数据中存在不平衡、缺失值、噪声数据。Huang证明SSL4NS-TD有在不同领域内保持鲁棒的表现的潜力，这将减少手工标注的小号。Lee使用了门控向量估计来自监督相关特征的选择过程，这可以避免选择冗余的特征并让更有信息的特征被学习。</p>
<h2 id="交叉表迁移性"><a href="#交叉表迁移性" class="headerlink" title="交叉表迁移性"></a>交叉表迁移性</h2><p>直接从表格学习表示需要对于每个下游数据集的训练模型，同时在测试数据与训练数据中也有着严格的特征限制。因此如何跨表格学习亟待解决。近期有许多方法实现了迁移性，包括基于PLM来从语义学角度上下文化知识的<strong>LIFT, TP-BERTa, GReaT</strong>；或者从头开始的细颗粒度特征编码器<strong>TransTab, XTab, UniTabE</strong>。这些方法证明：使用 SSL4NS-TD 进行预训练在适应增量列（incremental columns）、低资源场景（low-resource scenarios）以及缺失值预测（missing value predictions）方面具有优势。</p>
<h2 id="领域知识融合"><a href="#领域知识融合" class="headerlink" title="领域知识融合"></a>领域知识融合</h2><p>表格数据的应用常常需要有专家知识来推测结果，Du发现，使用地理图相关的特征作为前置任务是房产价格预测的重要因素。Nam设计了具有伪标签的自生成任务，该标签与下游标签有显著相关性（例如：通过地理位置和财产多少来估计房产价格类似于通过地理位置和财产来估计租金）。</p>
<h1 id="NS-TD数据集与Benchmarks"><a href="#NS-TD数据集与Benchmarks" class="headerlink" title="NS-TD数据集与Benchmarks"></a>NS-TD数据集与Benchmarks</h1><p>TabPFN表现最佳(这里的TabPFN是2023年的v1，2025年的v2准确率更高)，其次为SAINT，TabNet，VIME，TransTab.</p>
<h1 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h1><h2 id="SSL4NS-TD的秘籍"><a href="#SSL4NS-TD的秘籍" class="headerlink" title="SSL4NS-TD的秘籍"></a>SSL4NS-TD的秘籍</h2><p>除了已有的对于前置任务的探索：预测学习、对比学习、复合学习。SSL技术主要是来自于NLP与CV领域。目前仍然不清楚哪种SSL技术最佳、如何调超参。目前有一个很有前景的方向：就是如何设计与下游任务相关的前置任务。</p>
<h2 id="基础表格模型的进化"><a href="#基础表格模型的进化" class="headerlink" title="基础表格模型的进化"></a>基础表格模型的进化</h2><p>NLP的基础模型得到大大发展，但是基础模型依然未被完全探索。</p>
<h2 id="表格数据持续学习"><a href="#表格数据持续学习" class="headerlink" title="表格数据持续学习"></a>表格数据持续学习</h2><p>目前由于不同表格数据多样性与异质性，基础表格模型发展并不成熟。一种可能的方向就是统一表格数据格式，然后使用基础表格模型或者LLM处理。</p>
<h2 id="表格数据的隐私"><a href="#表格数据的隐私" class="headerlink" title="表格数据的隐私"></a>表格数据的隐私</h2><p>联邦学习是其中一种解决方法：多个设备和服务器同时训练模型且不共享本地数据，不上传数据，而是上传梯度或者权重。但是，由于上传的数据的质量和分布不一，所以联邦学习也存在问题。</p>
<h2 id="多模态多任务环境中的进展"><a href="#多模态多任务环境中的进展" class="headerlink" title="多模态多任务环境中的进展"></a>多模态多任务环境中的进展</h2><p>支持多任务学习能力（multi-task learning）可能有助于表格模型：<br>	在不同任务之间<strong>共享知识</strong>；<br>	同时以<strong>节省内存资源</strong>的方式来容纳这些任务。<br>进一步地，目前大多数的NS-TD方法依然仅关注于基于表格的数据，而忽视了多模态信息融合的潜力。然而，在 <strong>顺序表格数据领域（sequential tabular domain）</strong> 中，多模态融合（如将物品图片与其元数据结合，用于推荐系统）已经被证明是有效的。<br>此外，将表格格式的数据转换为文本格式（例如将数值“0”转化为“seen”，将“1”转化为“unseen”）能够利用大型语言模型（LLMs）中的通用知识来学习更丰富的上下文信息。<br>随着可用于训练的更大规模的表格数据集的增加；以及不同模态信息的融合技术的进步，我们认为：<strong>未来在这些方向上深入研究SSL在NS-TD中的应用（即SSL4NS-TD）</strong>，将有助于开发出<strong>更健壮、可部署性更强</strong>的学习方法。</p>
<hr>

<p>Cover image icon by <a target="_blank" rel="noopener" href="https://www.flaticon.com/free-icons/inference" title="inference icons">Dewi Sari</a> from <a target="_blank" rel="noopener" href="https://www.flaticon.com/free-icons/inference" title="inference icons">Flaticon</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://evanescence0515.github.io">Henry</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://evanescence0515.github.io/2025/08/07/Tabular-Review-2/">http://evanescence0515.github.io/2025/08/07/Tabular-Review-2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://evanescence0515.github.io" target="_blank">Evanescence's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a></div><div class="post-share"><div class="social-share" data-image="/img/deeplearning_pic.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/07/31/markdown%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/" title="markdown中的数学公式"><img class="cover" src="/img/epidemic_pic.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">markdown中的数学公式</div></div><div class="info-2"><div class="info-item-1">MathJax解析与Hexo冲突转义Hexo 中如需使用公式，需在 Markdown 开头需附上mathjax:true（如NexT主题）或math:true（如Fuild主题）。 一行 MathJax 公式中出现多个_{\rm a}_b{\rm a}_b: ${\rm a}{b}{\rm a}{b}${\rm a}\_b{\rm a}\b: ${\rm a}_b{\rm a}_b$欲显示Pax_Romana: $Pax_Romana$Pax\_Romana: $Pax\_Romana$欲换行：a\b\c：$a\b\c$a\\b\\c：$a\\b\\c$ 参考MathJax数学符号支持   Cover image icon by Dewi Sari from Flaticon </div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/07/31/AutoPNPNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="AutoPNPNet论文阅读"><img class="cover" src="/img/deeplearning_pic.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-31</div><div class="info-item-2">AutoPNPNet论文阅读</div></div><div class="info-2"><div class="info-item-1">模型速览 PNPNet(Periodic-Non-Periodic Network)和AutoPNPNet是作者提出的深度学习模型，用于挖掘数据内部的周期性来增强预测准确率。作者提出了FourierNet：一种基于Fourier的神经编码器来捕捉周期性特征；ChebyshevNet：使用Chebyshev神经编码器来建模非周期性特征。作者将这两种架构结合构成PNPNet和AutoPNPNet。模型效果优于SOTA（会有论文不是SOTA的吗？而且这里的SOTA是FT-Transformer(2021, Yury Gorishniy)）。   PNPNet detects periodic and non-periodic features a priori, feeding them into separate branches, while AutoPNPNet automatically selects features through a learned mechanism.   Q1：PNPNet如何使用滤波器与傅里叶变换获取periodic and non-periodi...</div></div></div></a><a class="pagination-related" href="/2025/07/27/TabICL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="TabICL论文阅读"><img class="cover" src="/img/deeplearning_pic.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-27</div><div class="info-item-2">TabICL论文阅读</div></div><div class="info-2"><div class="info-item-1">模型速览TabICL: In-Context Learning for Tabular Data是对于TabPFN的一次拓展，其名字取自于In-Context Learning(ICL)，主要是使用ICL对于数据量大于10K的表格数据进行预测。其可以处理500K的数据量的同时，速度是TabPFNv2的十倍。为了适配任意大小的表格数据，TabICL 将 单元格（cell） 作为基本处理单位：  每一列被看作是一组单元格值的集合，用于捕捉该特征的分布与语义；  每一行则由多个相互依赖的特征值构成。TabICL 采用两阶段架构，以实现高效的表格数据的 In-Context Learning（ICL，上下文学习）。 第一阶段：将每一行（不包含目标标签）编码为稠密向量（dense vector embeddings），每一个嵌入向量都被设计为能捕捉整个表格的信息。该阶段的本质是压缩列维度，从而显著降低后续 ICL 的计算复杂度与内存开销。 第二阶段：将这些紧凑但信息丰富的嵌入向量与其对应的标签结合，执行 ICL。因此，TabICL 的核心在于第一阶段的嵌入策略，它需要将行数据转换为具有语义...</div></div></div></a><a class="pagination-related" href="/2025/07/09/T2GFormer/" title="T2GFormer"><img class="cover" src="/img/deeplearning_pic.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-09</div><div class="info-item-2">T2GFormer</div></div><div class="info-2"><div class="info-item-1">Introduction作者在这篇文章中提出了一种新的特征交互方式。这种方式基于一种新型的图估计器，这种估计器可以自动估计特征之间的关系并通过连接相关特征来构造图像。一种特别的 跨层读取(Cross-level Readout) 收集了通过T2Gformer预测的不同层的突出特征并获取全局信息并作出预测。T2Gformer优于深度神经网络并与非深GBDT能力相近。T2Gformer的优势在于特征交互项的提取，所以作者在引入中便提及：已有的特征选择方法可以被分为“软”与”硬”版本。”软”版本本质上运用特征间的全连接交互项，例如乘法交互项、特征交叉与基于注意力的交互。但是表格特征本质上就带有异质性，所以全连接交互是一个次优的选项因为其盲目地将所有特征融合在一起。DANets使用了”硬”版本，其将相关的特征分组并将交互限制在被分组的特征内，虽然DANets取得了非常有潜力的成果，他的特征选取操作依旧不能解决组内交互的问题，因此同一组的特征被无差别的融合，使得模型表达能力下降。作者设计了Graph Estimator(GE)来组织表格特征为一个特征关联图(FR-Graph)，更进一步的，...</div></div></div></a><a class="pagination-related" href="/2025/05/08/CLIP/" title="CLIP"><img class="cover" src="/img/deeplearning_pic.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-08</div><div class="info-item-2">CLIP</div></div><div class="info-2"><div class="info-item-1">CLIP将计算机视觉与自然语言处理相结合，获得更加优秀的迁移性能与zero-shot效果。同时打破了固定标签的定式。 Abstract针对目前已有的计算机视觉系统，大部分都是使用固定的标签集合，这限制了它的泛化性能和可用性。于是作者选择通过图片的语言文本来进行图像识别。作者爬取了4亿张图片以进行模型的预训练。在预训练完成后，作者在30多个任务上进行了测试。在ImageNet数据集内，CLIP模型在zero-shot的情况下便已经与训练完成的Resnet50打成平手。 IntroductionGPT作为一个”Text-in-text-out”的经典案例，反映了弱监督工作的可行性，于是作者决定提出使用图片与文字结合，进行CLIP模型的研究。已有相关研究VirTex, ICMLM和ConVIRT方法虽然接近，但是数据集规模较少，而有些弱监督模型的准确率较高，其依赖的是极度大量数据集，所以作者考虑到是否能够同时满足以上条件，进而研究出新的方法。在预实验结果中，使用已有模型(ConVIRT)与新的数据，其模型在zero-shot上成功体现出极好的效果。同时模型效果与模型大小呈现正相关。 M...</div></div></div></a><a class="pagination-related" href="/2025/05/10/GNN/" title="GNN"><img class="cover" src="/img/deeplearning_pic.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-10</div><div class="info-item-2">GNN</div></div><div class="info-2"><div class="info-item-1">图是我们身边非常常见的结构，最近的一些研究让我们能够使用图结构的优势，在许多领域，如抗生素的研究、物理仿真、虚假新闻的识别，交通预测和推荐系统都有了最新的研究与实践。这篇学习&#x2F;总结博客参考了biliili上李沐的带读以及A Gentle Introduction to GNN这篇文章探索并解释了现代图神经网络，我们将工作分为4个部分：1. 什么类型的数据可以以图的形式记录；2. 图结构的特点与优势：相较于CNN或者传统深层神经网络；3. 建造一个GNN模型：从一个骨架到SOTA；4. 提供一个GNN playground供读者探究(会超链接回原文章)。 What a Graph is图是一系列实体之间的关系如果我们用图论的知识来看，图的描述其实有多种方式，如：邻接矩阵、集合的表示。我们这里采用相对容易接受的集合表示方法：一个图主要由 节点(Nodes) 和 边(Edges) 构成。以下图为例：其中存在5个节点，与6条边。其中点集合为$V &#x3D; { A,B,C,D,E }$。边集合为$E &#x3D; { AB, BC,CD,DE,AE, CE }$上图边为无向的...</div></div></div></a><a class="pagination-related" href="/2025/05/28/Tabular-Review-1/" title="表格数据学习方法综述阅读(上)"><img class="cover" src="/img/deeplearning_pic.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-28</div><div class="info-item-2">表格数据学习方法综述阅读(上)</div></div><div class="info-2"><div class="info-item-1">表格数据(tabular data)是日常生活中最常见的数据格式，为了挖掘表格数据中的有效信息，提出了很多相关的算法。这篇综述概括了表格数据的机器学习与深度学习方法，原文来自于南京大学LAMDA实验室中的LAMDA-tabular课题组，文章地址。 1-3对于tabular data的一些简单介绍，略过 4 特定方法的分类作者将模型分为三类：特化方法(specialized methods), 迁移方法(transferable methods), 广义方法(general methods)。 Specialized method作者从3个角度入手讲述特化模型的方法：Feature Aspect：特征角度来看，模型主要针对数据特征的关系进行建模。Sample Aspect：从样本特征来看，模型通过最大化每个样本最邻近样本的作用来预测。Objective Aspect：从客观角度，修改损失函数和整体目标(Overall objective)来引导模型特定的模式与偏好，注入inductive  bias(推断偏倚)。 Feature Aspect特征角度来看，模型主要针对数据特征的关...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/my_icon.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Henry</div><div class="author-info-description">至此鲜花赠自己，纵马踏花向自由</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/evanescence0515"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/evanescence0515" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:evanescence0515@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">你好呀，这里是Evanescence的博客，  我会经常分享一些学习统计学、生物信息学、机器学习和计算机技术的心得和笔记。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Problem-Definition-of-SSL4NS-TD"><span class="toc-number">2.</span> <span class="toc-text">Problem Definition of SSL4NS-TD</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Taxonomy"><span class="toc-number">3.</span> <span class="toc-text">Taxonomy</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.</span> <span class="toc-text">预测学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E9%81%AE%E6%8E%A9%E7%9A%84%E7%89%B9%E5%BE%81%E4%B8%AD%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.1.</span> <span class="toc-text">从遮掩的特征中学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BD%9C%E7%A9%BA%E9%97%B4%E7%9A%84%E6%89%B0%E5%8A%A8"><span class="toc-number">4.2.</span> <span class="toc-text">潜空间的扰动</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9B%BA%E6%9C%89%E8%83%BD%E5%8A%9B"><span class="toc-number">4.3.</span> <span class="toc-text">预训练语言模型的固有能力</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0"><span class="toc-number">5.</span> <span class="toc-text">对比学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%8D%E5%90%88%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.</span> <span class="toc-text">复合学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B2%E6%89%B0-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.1.</span> <span class="toc-text">干扰+对比学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A9%E7%A0%81-%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0"><span class="toc-number">6.2.</span> <span class="toc-text">掩码+对比学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SSL4NS-TD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">7.</span> <span class="toc-text">SSL4NS-TD的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B"><span class="toc-number">7.1.</span> <span class="toc-text">自动数据工程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E8%A1%A8%E8%BF%81%E7%A7%BB%E6%80%A7"><span class="toc-number">7.2.</span> <span class="toc-text">交叉表迁移性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E8%9E%8D%E5%90%88"><span class="toc-number">7.3.</span> <span class="toc-text">领域知识融合</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NS-TD%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8EBenchmarks"><span class="toc-number">8.</span> <span class="toc-text">NS-TD数据集与Benchmarks</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91"><span class="toc-number">9.</span> <span class="toc-text">未来方向</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#SSL4NS-TD%E7%9A%84%E7%A7%98%E7%B1%8D"><span class="toc-number">9.1.</span> <span class="toc-text">SSL4NS-TD的秘籍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E8%A1%A8%E6%A0%BC%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%BF%9B%E5%8C%96"><span class="toc-number">9.2.</span> <span class="toc-text">基础表格模型的进化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0"><span class="toc-number">9.3.</span> <span class="toc-text">表格数据持续学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%A8%E6%A0%BC%E6%95%B0%E6%8D%AE%E7%9A%84%E9%9A%90%E7%A7%81"><span class="toc-number">9.4.</span> <span class="toc-text">表格数据的隐私</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%9A%E4%BB%BB%E5%8A%A1%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84%E8%BF%9B%E5%B1%95"><span class="toc-number">9.5.</span> <span class="toc-text">多模态多任务环境中的进展</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/08/07/Tabular-Review-2/" title="Tabular-Review-2"><img src="/img/deeplearning_pic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Tabular-Review-2"/></a><div class="content"><a class="title" href="/2025/08/07/Tabular-Review-2/" title="Tabular-Review-2">Tabular-Review-2</a><time datetime="2025-08-07T11:12:31.000Z" title="发表于 2025-08-07 19:12:31">2025-08-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/31/markdown%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/" title="markdown中的数学公式"><img src="/img/epidemic_pic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="markdown中的数学公式"/></a><div class="content"><a class="title" href="/2025/07/31/markdown%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/" title="markdown中的数学公式">markdown中的数学公式</a><time datetime="2025-07-31T14:59:16.000Z" title="发表于 2025-07-31 22:59:16">2025-07-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/31/AutoPNPNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="AutoPNPNet论文阅读"><img src="/img/deeplearning_pic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AutoPNPNet论文阅读"/></a><div class="content"><a class="title" href="/2025/07/31/AutoPNPNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="AutoPNPNet论文阅读">AutoPNPNet论文阅读</a><time datetime="2025-07-31T14:21:32.000Z" title="发表于 2025-07-31 22:21:32">2025-07-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/27/TabICL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="TabICL论文阅读"><img src="/img/deeplearning_pic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="TabICL论文阅读"/></a><div class="content"><a class="title" href="/2025/07/27/TabICL%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="TabICL论文阅读">TabICL论文阅读</a><time datetime="2025-07-27T15:06:12.000Z" title="发表于 2025-07-27 23:06:12">2025-07-27</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/16/%E5%AE%9E%E7%94%A8%E4%BB%A3%E7%A0%81%EF%BC%88%E9%9A%8F%E6%9C%BA%E6%9B%B4%E6%96%B0%EF%BC%89/" title="实用代码（随机更新）"><img src="/img/epidemic_pic.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="实用代码（随机更新）"/></a><div class="content"><a class="title" href="/2025/07/16/%E5%AE%9E%E7%94%A8%E4%BB%A3%E7%A0%81%EF%BC%88%E9%9A%8F%E6%9C%BA%E6%9B%B4%E6%96%B0%EF%BC%89/" title="实用代码（随机更新）">实用代码（随机更新）</a><time datetime="2025-07-16T08:26:46.000Z" title="发表于 2025-07-16 16:26:46">2025-07-16</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;2019 - 2025 By Henry</span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索内容..." type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>